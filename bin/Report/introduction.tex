\section{Introduction} 
\label {chapter:introduction}
Computers are able to perform complex computations, which allows for solving problems which would be impossible to solve by hand. However, computers need to be programmed before they can compute anything. Theoretically, anything can be programmed, but some problems would require extreme amounts of code to be written. One such problem is classifying certain continuous inputs into classes. To overcome this problem, it is possible to make the computer learn by itself. This is done by chaining large numbers of deterministic perceptrons, which take one or more inputs and give a certain output, determined by weights. By adjusting these weights, it is possible to reach the desired output. 
The assignment given was to classify products into 7 categories, based on 10 features of these products. Because a perfect score is not realistic, the challenge is to make the errors made in classifying as small as possible. To achieve this, different numbers of hidden layers and neurons were tested.
Chapter \ref{chapter:architecture} answers five questions that were posed to test the understanding of the problem and the solution. Chapter \ref{chapter:training} describes the theory of how to optimize the network. Chapter \ref{chapter:optimization} then describes the execution of this optimization. The outcome of the network is described in Chapter \ref{chapter:evaluation}, and lastly in Chapter \ref{chapter:matlabtoolbox} the outcome is compared to that of the toolbox which is built in in MatLab.